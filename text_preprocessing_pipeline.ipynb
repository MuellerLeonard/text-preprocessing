{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRkgRT5oBhHCIGUEF1rBy5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuellerLeonard/text-preprocessing/blob/main/text_preprocessing_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "High RAM needed"
      ],
      "metadata": {
        "id": "Cy7R1zZKDoc2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEfuyhtnDZW9",
        "outputId": "593b9beb-b4d2-497a-a6fc-7e71e9851b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone git\n",
        "\n"
      ],
      "metadata": {
        "id": "iSD1YVbL3vA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MuellerLeonard/text-preprocessing.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p9_j0YR35dK",
        "outputId": "09069df3-1993-465a-f2e1-79e6460c080d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'text-preprocessing' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls text-preprocessing/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUejZwMSOMUg",
        "outputId": "2040ac3a-0fd1-4e5e-b561-b76499b33020"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed_text_df_created.csv  text_corpus_clean_created.txt\n",
            "processed_text_df.csv\t       text_corpus_clean.txt\n",
            "README.md\t\t       text_corpus_created.txt\n",
            "showcase.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "fGR4DayTD0xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "kSo6fLz4DyX_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the Dataset"
      ],
      "metadata": {
        "id": "lBbtoY7ND7ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/text-preprocessing/showcase.csv\", nrows=100)\n",
        "df.columns.values[0] = 'Date'"
      ],
      "metadata": {
        "id": "FLxUx3-KD3DV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test if DataFrame is correct\n",
        "\n",
        "dri_df = df[df['Ticker'] == 'DRI']\n",
        "print(dri_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdq80JpeEi_h",
        "outputId": "b92cfbd4-1b2f-4ff6-cffb-9e51bc845905"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Date, Unnamed: 0, Adj Close, item 5, item 4, item 5.02, item_1, item_1a, item 7, item 8.01, part_1_item_2, part_2_item_1a, net income, item 9.01, Ticker, Time, Filing, 10, Stock prices before (np.ndarray), Stock prices after (np.ndarray), Stock movement 1, Stock movement 7, Stock movement 30, 1 prior Adj Close, 1 prior Stock movement 1, 1 prior net income, 1 prior total stockholders’ equity, 1 prior retained earnings, 1 prior total operating expenses, 1 prior interest expense, 1 prior research and development, 1 prior net worth, 1 prior total assets, 1 prior inventories, 1 prior long-term debt, 1 prior short-term debt, 1 prior total net sales, 1 prior total liabilities and stockholders’ equity, 1 prior other income (expense), net, 2 prior Adj Close, 2 prior Stock movement 1, 2 prior net income, 2 prior total stockholders’ equity, 2 prior retained earnings, 2 prior total operating expenses, 2 prior interest expense, 2 prior research and development, 2 prior net worth, 2 prior total assets, 2 prior inventories, 2 prior long-term debt, 2 prior short-term debt, 2 prior total net sales, 2 prior total liabilities and stockholders’ equity, 2 prior other income (expense), net, 3 prior Adj Close, 3 prior Stock movement 1, 3 prior net income, 3 prior total stockholders’ equity, 3 prior retained earnings, 3 prior total operating expenses, 3 prior interest expense, 3 prior research and development, 3 prior net worth, 3 prior total assets, 3 prior inventories, 3 prior long-term debt, 3 prior short-term debt, 3 prior total net sales, 3 prior total liabilities and stockholders’ equity, 3 prior other income (expense), net, 4 prior Adj Close, 4 prior Stock movement 1, 4 prior net income, 4 prior total stockholders’ equity, 4 prior retained earnings, 4 prior total operating expenses, 4 prior interest expense, 4 prior research and development, 4 prior net worth, 4 prior total assets, 4 prior inventories, 4 prior long-term debt, 4 prior short-term debt, 4 prior total net sales, 4 prior total liabilities and stockholders’ equity, 4 prior other income (expense), net, 5 prior Adj Close, 5 prior Stock movement 1, 5 prior net income, 5 prior total stockholders’ equity, 5 prior retained earnings, 5 prior total operating expenses, 5 prior interest expense, 5 prior research and development, 5 prior net worth, 5 prior total assets, 5 prior inventories, 5 prior long-term debt, 5 prior short-term debt, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 164 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drop and Arrange the Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "QWXwjuw6D_z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "T0Tcd7tXJ4aI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop Cells that are not needed"
      ],
      "metadata": {
        "id": "UZalLk6iEu9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = df.columns\n",
        "\n",
        "print(columns)\n",
        "\n",
        "cleaned_columns = []\n",
        "\n",
        "to_drop = ['item_1', 'item_1a', 'part_1_item_2', 'part_2_item_1a', 'item 1', 'item 2', 'item 3', 'item 4', 'item 5', 'item 6', 'item 7', 'item 8', 'item 9', 'item 12', 'item 601', 'item 8.1']\n",
        "loop_drop = ['1 prior', '2 prior', '3 prior', '4 prior', '5 prior', '6 prior']\n",
        "dropped = []\n",
        "\n",
        "for dropit in to_drop:\n",
        "  for drop in loop_drop:\n",
        "    dropped.append(drop + ' ' + dropit)\n",
        "\n",
        "print(\"DROPPED: \", dropped)\n",
        "\n",
        "cleaned_columns = [col for col in columns if col not in dropped]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z2kQBruEgdi",
        "outputId": "8d3cedc3-2008-4606-bd30-b890f08ac90d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Date', 'Unnamed: 0', 'Adj Close', 'item 5', 'item 4', 'item 5.02',\n",
            "       'item_1', 'item_1a', 'item 7', 'item 8.01',\n",
            "       ...\n",
            "       'item 1.04', 'item 1.05', 'item 5.08', 'item 5.05', 'item 601',\n",
            "       'item 8.1', 'item 4.02', 'item 2', 'item 4.01', 'item 3.03'],\n",
            "      dtype='object', length=164)\n",
            "DROPPED:  ['1 prior item_1', '2 prior item_1', '3 prior item_1', '4 prior item_1', '5 prior item_1', '6 prior item_1', '1 prior item_1a', '2 prior item_1a', '3 prior item_1a', '4 prior item_1a', '5 prior item_1a', '6 prior item_1a', '1 prior part_1_item_2', '2 prior part_1_item_2', '3 prior part_1_item_2', '4 prior part_1_item_2', '5 prior part_1_item_2', '6 prior part_1_item_2', '1 prior part_2_item_1a', '2 prior part_2_item_1a', '3 prior part_2_item_1a', '4 prior part_2_item_1a', '5 prior part_2_item_1a', '6 prior part_2_item_1a', '1 prior item 1', '2 prior item 1', '3 prior item 1', '4 prior item 1', '5 prior item 1', '6 prior item 1', '1 prior item 2', '2 prior item 2', '3 prior item 2', '4 prior item 2', '5 prior item 2', '6 prior item 2', '1 prior item 3', '2 prior item 3', '3 prior item 3', '4 prior item 3', '5 prior item 3', '6 prior item 3', '1 prior item 4', '2 prior item 4', '3 prior item 4', '4 prior item 4', '5 prior item 4', '6 prior item 4', '1 prior item 5', '2 prior item 5', '3 prior item 5', '4 prior item 5', '5 prior item 5', '6 prior item 5', '1 prior item 6', '2 prior item 6', '3 prior item 6', '4 prior item 6', '5 prior item 6', '6 prior item 6', '1 prior item 7', '2 prior item 7', '3 prior item 7', '4 prior item 7', '5 prior item 7', '6 prior item 7', '1 prior item 8', '2 prior item 8', '3 prior item 8', '4 prior item 8', '5 prior item 8', '6 prior item 8', '1 prior item 9', '2 prior item 9', '3 prior item 9', '4 prior item 9', '5 prior item 9', '6 prior item 9', '1 prior item 12', '2 prior item 12', '3 prior item 12', '4 prior item 12', '5 prior item 12', '6 prior item 12', '1 prior item 601', '2 prior item 601', '3 prior item 601', '4 prior item 601', '5 prior item 601', '6 prior item 601', '1 prior item 8.1', '2 prior item 8.1', '3 prior item 8.1', '4 prior item 8.1', '5 prior item 8.1', '6 prior item 8.1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop Rows that can not be experimented with"
      ],
      "metadata": {
        "id": "0guD5mcqE3wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop rows with NaN inside a label column\n",
        "\n",
        "df = df[(df[\"Adj Close\"].notna()) & (df[\"Stock movement 1\"].notna())]\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(df.head(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI9nokmfE6Zu",
        "outputId": "46844165-5412-4652-bde2-543771ee2ab5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Date  Unnamed: 0  Adj Close item 5 item 4 item 5.02 item_1 item_1a item 7  \\\n",
            "0    95  2019-03-05  76.549049    NaN    NaN       NaN    NaN     NaN    NaN   \n",
            "\n",
            "  item 8.01  ... item 1.04 item 1.05  item 5.08 item 5.05 item 601 item 8.1  \\\n",
            "0       NaN  ...       NaN       NaN        NaN       NaN      NaN      NaN   \n",
            "\n",
            "   item 4.02 item 2 item 4.01 item 3.03  \n",
            "0        NaN    NaN       NaN       NaN  \n",
            "\n",
            "[1 rows x 164 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add cells that might be interesting"
      ],
      "metadata": {
        "id": "byseWFaUFIUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adjusted Close Price of the next day"
      ],
      "metadata": {
        "id": "sWfpUwRWFPaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_price_next_day(row):\n",
        "  \"\"\"\n",
        "  add next day price column to DataFrame\n",
        "\n",
        "  Args:\n",
        "    - row (pd.Series): row of a DataFrame\n",
        "\n",
        "  Returns:\n",
        "    - value[1] (np.float): next day price value\n",
        "  \"\"\"\n",
        "  value = np.fromstring(row['Stock prices after (np.ndarray)'].strip(\"[]\"), sep=' ') #.fromstring(dtype=float)\n",
        "  return value[1]\n",
        "\n",
        "df['Adj Close next day'] = df.apply(get_price_next_day, axis=1)\n",
        "\n",
        "cleaned_columns = cleaned_columns + ['Adj Close next day']\n",
        "\n",
        "print(df.head(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsmkI01bE8vq",
        "outputId": "3fe9d26e-57d0-4d7f-dfad-624611a3ee90"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Date  Unnamed: 0  Adj Close item 5 item 4 item 5.02 item_1 item_1a item 7  \\\n",
            "0    95  2019-03-05  76.549049    NaN    NaN       NaN    NaN     NaN    NaN   \n",
            "\n",
            "  item 8.01  ... item 1.05 item 5.08  item 5.05 item 601 item 8.1 item 4.02  \\\n",
            "0       NaN  ...       NaN       NaN        NaN      NaN      NaN       NaN   \n",
            "\n",
            "   item 2 item 4.01 item 3.03 Adj Close next day  \n",
            "0     NaN       NaN       NaN          76.549049  \n",
            "\n",
            "[1 rows x 165 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arrange items\n",
        "\n",
        "- **eightk_items** -> List: includes all columns with 8K text\n",
        "- **tenk_text_items** -> List: includes all columns with 10K text (Management Discussion, Risk analysis)\n",
        "- **tenq_text_items** -> List: includes all columns with 10Q text (Management Discussion, Risk analysis)\n",
        "- **tenkq_numerical_items** -> List: includes all the numerical data found in the 10K/Q Filings\n",
        "- **metadata_items** -> List: includes all columns with metadata (Labels etc.)\n",
        "- **timeseries_items** -> List: includes all columns with timeseries data for the corresponding company"
      ],
      "metadata": {
        "id": "rGVDU2yFFXHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eightk_items = []\n",
        "tenk_text_items = ['item_1', 'item_1a']\n",
        "tenq_text_items = ['part_1_item_2', 'part_2_item_1a']\n",
        "tenkq_numerical_items = []\n",
        "metadata_items = ['Adj Close', 'Stock movement 1', 'Stock movement 7', 'Stock movement 30', 'Filing', 'Time', '10', 'Ticker', 'Date']\n",
        "timeseries_items = ['Stock prices before (np.ndarray)', 'Stock prices after (np.ndarray)']\n",
        "\n",
        "item_pattern = re.compile(r\"^(Item\\s[1-9][\\.\\d]*)\", re.IGNORECASE)\n",
        "\n",
        "for i in range(len(cleaned_columns)):\n",
        "  match = item_pattern.match(cleaned_columns[i])\n",
        "  if match:\n",
        "    eightk_items.append(match.group(0))\n",
        "\n",
        "print(eightk_items)\n",
        "print(len(eightk_items))\n",
        "\n",
        "other = eightk_items + tenk_text_items + tenq_text_items + metadata_items + timeseries_items\n",
        "\n",
        "print(\"OTHER:\",other)\n",
        "\n",
        "for col in cleaned_columns:\n",
        "  if col not in other:\n",
        "    tenkq_numerical_items.append(col)\n",
        "\n",
        "print(\"TENKQNUM:\", tenkq_numerical_items)\n",
        "\n",
        "df = df[cleaned_columns]\n",
        "\n",
        "print(df.head(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m4tpd2NFZi_",
        "outputId": "2ea5d89b-e577-4041-e780-febdd7d8d622"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['item 5', 'item 4', 'item 5.02', 'item 7', 'item 8.01', 'item 9.01', 'item 7.01', 'item 5.07', 'item 2.02', 'item 3.02', 'item 2.03', 'item 1.01', 'item 1.02', 'item 2.04', 'item 2.06', 'item\\u202f9.01', 'item 3.01', 'item 3', 'item 5.04', 'item 5.03', 'item 9', 'item 2.05', 'item 8', 'item 2.01', 'item 5.01', 'item 1', 'item 6', 'item 12', 'item 1.04', 'item 1.05', 'item 5.08', 'item 5.05', 'item 601', 'item 8.1', 'item 4.02', 'item 2', 'item 4.01', 'item 3.03']\n",
            "38\n",
            "OTHER: ['item 5', 'item 4', 'item 5.02', 'item 7', 'item 8.01', 'item 9.01', 'item 7.01', 'item 5.07', 'item 2.02', 'item 3.02', 'item 2.03', 'item 1.01', 'item 1.02', 'item 2.04', 'item 2.06', 'item\\u202f9.01', 'item 3.01', 'item 3', 'item 5.04', 'item 5.03', 'item 9', 'item 2.05', 'item 8', 'item 2.01', 'item 5.01', 'item 1', 'item 6', 'item 12', 'item 1.04', 'item 1.05', 'item 5.08', 'item 5.05', 'item 601', 'item 8.1', 'item 4.02', 'item 2', 'item 4.01', 'item 3.03', 'item_1', 'item_1a', 'part_1_item_2', 'part_2_item_1a', 'Adj Close', 'Stock movement 1', 'Stock movement 7', 'Stock movement 30', 'Filing', 'Time', '10', 'Ticker', 'Date', 'Stock prices before (np.ndarray)', 'Stock prices after (np.ndarray)']\n",
            "TENKQNUM: ['Unnamed: 0', 'net income', '1 prior Adj Close', '1 prior Stock movement 1', '1 prior net income', '1 prior total stockholders’ equity', '1 prior retained earnings', '1 prior total operating expenses', '1 prior interest expense', '1 prior research and development', '1 prior net worth', '1 prior total assets', '1 prior inventories', '1 prior long-term debt', '1 prior short-term debt', '1 prior total net sales', '1 prior total liabilities and stockholders’ equity', '1 prior other income (expense), net', '2 prior Adj Close', '2 prior Stock movement 1', '2 prior net income', '2 prior total stockholders’ equity', '2 prior retained earnings', '2 prior total operating expenses', '2 prior interest expense', '2 prior research and development', '2 prior net worth', '2 prior total assets', '2 prior inventories', '2 prior long-term debt', '2 prior short-term debt', '2 prior total net sales', '2 prior total liabilities and stockholders’ equity', '2 prior other income (expense), net', '3 prior Adj Close', '3 prior Stock movement 1', '3 prior net income', '3 prior total stockholders’ equity', '3 prior retained earnings', '3 prior total operating expenses', '3 prior interest expense', '3 prior research and development', '3 prior net worth', '3 prior total assets', '3 prior inventories', '3 prior long-term debt', '3 prior short-term debt', '3 prior total net sales', '3 prior total liabilities and stockholders’ equity', '3 prior other income (expense), net', '4 prior Adj Close', '4 prior Stock movement 1', '4 prior net income', '4 prior total stockholders’ equity', '4 prior retained earnings', '4 prior total operating expenses', '4 prior interest expense', '4 prior research and development', '4 prior net worth', '4 prior total assets', '4 prior inventories', '4 prior long-term debt', '4 prior short-term debt', '4 prior total net sales', '4 prior total liabilities and stockholders’ equity', '4 prior other income (expense), net', '5 prior Adj Close', '5 prior Stock movement 1', '5 prior net income', '5 prior total stockholders’ equity', '5 prior retained earnings', '5 prior total operating expenses', '5 prior interest expense', '5 prior research and development', '5 prior net worth', '5 prior total assets', '5 prior inventories', '5 prior long-term debt', '5 prior short-term debt', '5 prior total net sales', '5 prior total liabilities and stockholders’ equity', '5 prior other income (expense), net', '6 prior Adj Close', '6 prior Stock movement 1', '6 prior net income', '6 prior total stockholders’ equity', '6 prior retained earnings', '6 prior total operating expenses', '6 prior interest expense', '6 prior research and development', '6 prior net worth', '6 prior total assets', '6 prior inventories', '6 prior long-term debt', '6 prior short-term debt', '6 prior total net sales', '6 prior total liabilities and stockholders’ equity', '6 prior other income (expense), net', 'total stockholders’ equity', 'retained earnings', 'total operating expenses', 'interest expense', 'research and development', 'net worth', 'total assets', 'inventories', 'long-term debt', 'short-term debt', 'total net sales', 'total liabilities and stockholders’ equity', 'other income (expense), net', 'Adj Close next day']\n",
            "   Date  Unnamed: 0  Adj Close item 5 item 4 item 5.02 item_1 item_1a item 7  \\\n",
            "0    95  2019-03-05  76.549049    NaN    NaN       NaN    NaN     NaN    NaN   \n",
            "\n",
            "  item 8.01  ... item 1.05 item 5.08  item 5.05 item 601 item 8.1 item 4.02  \\\n",
            "0       NaN  ...       NaN       NaN        NaN      NaN      NaN       NaN   \n",
            "\n",
            "   item 2 item 4.01 item 3.03 Adj Close next day  \n",
            "0     NaN       NaN       NaN          76.549049  \n",
            "\n",
            "[1 rows x 165 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text\n",
        "\n",
        "Preprocessing Text columns\n",
        "\n",
        "- Reference: https://doi.org/10.1016/j.dss.2022.113892"
      ],
      "metadata": {
        "id": "_RwoWbEKF3Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eightk_items\n",
        "tenk_text_items\n",
        "tenq_text_items\n",
        "#label_columns = ['Adj Close', 'Stock movement 1', 'Stock movement 7', 'Stock movement 30', 'Adj Close next day', 'Filing', 'Time', '10', 'Ticker', 'Date']\n",
        "\n",
        "\n",
        "text_columns = eightk_items + tenk_text_items + tenq_text_items + metadata_items\n",
        "\n",
        "text_df = df[text_columns]\n",
        "\n",
        "print(text_df.head(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0b9NH0mFnDD",
        "outputId": "f9f5198c-97bb-41c0-f8d1-c42875f8495e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  item 5 item 4 item 5.02 item 7 item 8.01 item 9.01  item 7.01 item 5.07  \\\n",
            "0    NaN    NaN       NaN    NaN       NaN       NaN        NaN       NaN   \n",
            "\n",
            "   item 2.02  item 3.02  ...  \\\n",
            "0        NaN        NaN  ...   \n",
            "\n",
            "                                      part_2_item_1a  Adj Close  \\\n",
            "0  ITEM 1A. RISK FACTORS\\nRisks, Uncertainties an...  76.549049   \n",
            "\n",
            "   Stock movement 1  Stock movement 7  Stock movement 30  Filing      Time  \\\n",
            "0         75.598679         77.384247            74.5989    True  16:08:06   \n",
            "\n",
            "     10  Ticker  Date  \n",
            "0  True       A    95  \n",
            "\n",
            "[1 rows x 51 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get a complete text corpus\n",
        "\n",
        "- input: Dataframe with only text columns"
      ],
      "metadata": {
        "id": "OY8pL04UHHQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_path = \"/content/text-preprocessing/text_corpus_generated.txt\""
      ],
      "metadata": {
        "id": "WQKmXaM1zkkk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select text columns automatically:\n",
        "# text_cols = text_df.select_dtypes(include=['object', 'string']).columns\n",
        "\n",
        "text_df.fillna('').to_csv(raw_path, sep = '|', index=False, header=False)"
      ],
      "metadata": {
        "id": "KWIUzcm5IxIm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clean the text corpus partially\n",
        "\n",
        "Goal:\n",
        "- finding the rare words\n",
        "- and common phrases\n",
        "\n",
        "without to much noise"
      ],
      "metadata": {
        "id": "wS-ukfKBV0tN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Replace abbreviations after: '\n",
        "- t: to not\n",
        "- s: append to the word coming before (either plural or is)\n",
        "- ve: have\n",
        "- d: would\n",
        "- m: am\n",
        "- ll: will\n",
        "- re: are\n",
        "- doesn't: does not\n",
        "- can't: can not\n",
        "- won't: will not\n",
        "- don't: do not\n",
        "- I've: i have\n",
        "- I'd: i would\n",
        "- I'm: i am\n",
        "- I'll: i will\n",
        "- She's: she is\n",
        "- He's: he is\n",
        "- It's: it is\n",
        "- there's: there is\n",
        "- they're: they are\n",
        "- we're: we are\n",
        "- you've: you have\n",
        "- you're: you are\n",
        "- couldn't: could not\n",
        "- shouldn't: should not\n",
        "- wouldn't: would not"
      ],
      "metadata": {
        "id": "eRWmXbbyWJr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_abbreviations(text):\n",
        "  # Define contraction patterns and their replacements\n",
        "  contractions = {\n",
        "    r\"\\bdoesn't\\b\": \"does not\",\n",
        "    r\"\\bcan't\\b\": \"can not\",\n",
        "    r\"\\bwon't\\b\": \"will not\",\n",
        "    r\"\\bdon't\\b\": \"do not\",\n",
        "    r\"\\bI've\\b\": \"i have\",\n",
        "    r\"\\bI'd\\b\": \"i would\",\n",
        "    r\"\\bI'm\\b\": \"i am\",\n",
        "    r\"\\bI'll\\b\": \"i will\",\n",
        "    r\"\\bShe's\\b\": \"she is\",\n",
        "    r\"\\bHe's\\b\": \"he is\",\n",
        "    r\"\\bIt's\\b\": \"it is\",\n",
        "    r\"\\bthere's\\b\": \"there is\",\n",
        "    r\"\\bthey're\\b\": \"they are\",\n",
        "    r\"\\bwe're\\b\": \"we are\",\n",
        "    r\"\\byou've\\b\": \"you have\",\n",
        "    r\"\\byou're\\b\": \"you are\",\n",
        "    r\"\\bcouldn't\\b\": \"could not\",\n",
        "    r\"\\bshouldn't\\b\": \"should not\",\n",
        "    r\"\\bwouldn't\\b\": \"would not\",\n",
        "  }\n",
        "\n",
        "  for pattern, replacement in contractions.items():\n",
        "    text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "5O3YRLQ5V305"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Remove contact details"
      ],
      "metadata": {
        "id": "aRGI6UVrWQFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_contact_details(text):\n",
        "  # Regular expression patterns\n",
        "  email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "  phone_pattern = r'(\\+?\\d{1,3}[-.\\s]?)?(\\(?\\d{3}\\)?[-.\\s]?)?[\\d.\\s-]{7,}'\n",
        "  url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "  social_media_pattern = r'@[a-zA-Z0-9_]+'\n",
        "\n",
        "  # Substitute patterns with an empty string\n",
        "  text = re.sub(email_pattern, '', text)          # Remove email addresses\n",
        "  text = re.sub(phone_pattern, '', text)          # Remove phone numbers\n",
        "  text = re.sub(url_pattern, '', text)            # Remove URLs\n",
        "  text = re.sub(social_media_pattern, '', text)   # Remove social media handles\n",
        "\n",
        "  # Remove any extra whitespace that might result from the removals\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "0-maLDCpWTaa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Remove stop words\n",
        "\n",
        "- Reference: https://doi.org/10.1145/2783258.278338\n",
        "- Cite: Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O’Reilly Media Inc."
      ],
      "metadata": {
        "id": "r8kd_9_8WWkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "BPnFmaSBWYIM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2csf5-5fWgew",
        "outputId": "e2558905-928d-4d68-a488-48337a761bde"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_stop_word_list():\n",
        "  \"\"\"\n",
        "  write List with stop words from file\n",
        "\n",
        "  Returns:\n",
        "    - stop_words (List[]): List with stop words\n",
        "  \"\"\"\n",
        "  with open(\"/content/text-preprocessing/StopWords_Generic.txt\", \"r\") as f:\n",
        "    stop_words = [line.strip().lower() for line in f]\n",
        "\n",
        "    return stop_words"
      ],
      "metadata": {
        "id": "99cV7w5CWjpj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_and_punctuation(text):\n",
        "  \"\"\"\n",
        "  remove stop words and punctuation from a string\n",
        "\n",
        "  Args:\n",
        "    - text (string): text input\n",
        "\n",
        "  Returns:\n",
        "    - filtered_text (string): text without punctuation and stop words\n",
        "  \"\"\"\n",
        "\n",
        "  # convert to tokens\n",
        "  word_tokens = word_tokenize(text)\n",
        "  stop_words_loughran = read_stop_word_list()\n",
        "\n",
        "  filtered_text = [w for w in word_tokens if w not in stop_words_loughran]\n",
        "\n",
        "  filtered_text = [w for w in word_tokens if w.isalpha()]\n",
        "\n",
        "  return filtered_text"
      ],
      "metadata": {
        "id": "v9l13Xx5Wn1S"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "  \"\"\"\n",
        "  remove the punctuation from a string\n",
        "\n",
        "  Args:\n",
        "    - text (string): input string\n",
        "\n",
        "  Returns:\n",
        "    - filtered_text (string): string without punctuation, except for elements in sentence_punctuation\n",
        "  \"\"\"\n",
        "  # keep sentence punctuation for Word2Vec methods\n",
        "  sentence_punctuation = ['.', '!', '?', ':', ';', '\\n']\n",
        "  # convert to tokens\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_text = [w for w in word_tokens if w.isalpha() or w in sentence_punctuation]\n",
        "\n",
        "  return filtered_text"
      ],
      "metadata": {
        "id": "xWhodQRVWo40"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### append plural or genitive \"s\" to the word"
      ],
      "metadata": {
        "id": "2NqWIz-qWslW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def move_s_back(tokens):\n",
        "  \"\"\"\n",
        "  standalone s' are appended to the previous word (due to removed punctuation)\n",
        "\n",
        "  Args:\n",
        "    - tokens (List[]): List of word tokens\n",
        "\n",
        "  Returns:\n",
        "    - result (List[]): List of word tokens with the s appended\n",
        "  \"\"\"\n",
        "  result = []\n",
        "  i = 0\n",
        "  while i < len(tokens):\n",
        "    if tokens[i] == \"s\" and i > 0:\n",
        "      # Append \"s\" to the previous word in the result list\n",
        "      result[-1] += \"s\"\n",
        "    else:\n",
        "      # Add the token to the result list as-is\n",
        "      result.append(tokens[i])\n",
        "    i += 1\n",
        "  return result"
      ],
      "metadata": {
        "id": "eTNatJ0XWyUG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Replace positive and negative numbers and dates"
      ],
      "metadata": {
        "id": "xJxSir-mW0BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dateutil import parser"
      ],
      "metadata": {
        "id": "LiPrOHfvagtV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_nums_dates(text):\n",
        "  \"\"\"\n",
        "  replace numbers and dates with a placeholder\n",
        "\n",
        "  Args:\n",
        "    - text (string): input string\n",
        "\n",
        "  Returns:\n",
        "    - text (string): string with replacements\n",
        "  \"\"\"\n",
        "  number_pattern = r'\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b'\n",
        "  date_pattern = r'\\b(?:\\d{1,2}[-/])?(?:\\d{1,2}[-/])?\\d{2,4}\\b|\\b\\d{4}\\b'\n",
        "\n",
        "  text = re.sub(number_pattern, \"NUMBER\", text)\n",
        "  text = re.sub(date_pattern, \"DATE\", text)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "GiC-bsbdXQ7R"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create the partially cleaned text corpus\n",
        "\n",
        "- tokenize"
      ],
      "metadata": {
        "id": "L8Rr6plvZwPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_corpus_cleaned(text, tokens):\n",
        "  \"\"\"\n",
        "  calls preprocessing functions\n",
        "\n",
        "  Args:\n",
        "    - text (string): input string\n",
        "    - tokens (boolean): set True or False depending on returntype\n",
        "\n",
        "  Returns:\n",
        "    - token_list (List[string]): List of tokens\n",
        "    - result_text (string): text\n",
        "  \"\"\"\n",
        "  text = remove_contact_details(text)\n",
        "  text = replace_nums_dates(text)\n",
        "\n",
        "  # lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # ignores case\n",
        "  text = replace_abbreviations(text)\n",
        "\n",
        "  # Or remove_stopwords_and_punctuation, does not work, out of personal experience reduces performance for some models\n",
        "  token_list = remove_punctuation(text)\n",
        "  token_list = move_s_back(token_list)\n",
        "\n",
        "  if tokens:\n",
        "    return token_list\n",
        "  else:\n",
        "    result_text = \" \".join(token_list)\n",
        "    return result_text"
      ],
      "metadata": {
        "id": "frIfQTxVZ1ft"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_text_corpus(path):\n",
        "  \"\"\"\n",
        "  read from file\n",
        "\n",
        "  Args:\n",
        "    - path (string): path to file\n",
        "\n",
        "  Returns:\n",
        "    - text (string): file content\n",
        "  \"\"\"\n",
        "  with open(path, \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "nUEAenrhb-XI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = read_text_corpus(path=\"/content/text-preprocessing/text_corpus_generated.txt\")\n",
        "\n",
        "clean_tokens = create_corpus_cleaned(text, tokens=True)"
      ],
      "metadata": {
        "id": "JdvRih3VbZNj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Prerequisits for further preprocessing\n",
        "\n",
        "extract from the text corpus:\n",
        "\n",
        "- common phrases\n",
        "- rare words"
      ],
      "metadata": {
        "id": "rx4VDcxhJQS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "from collections import Counter\n",
        "import string"
      ],
      "metadata": {
        "id": "N-hA2bvYJkTB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Find common phrases\n",
        "\n",
        "possible improvement:\n",
        "- calculate the distance between the phrases (levenstein, cos distance)\n",
        "- group them, depending on the distance"
      ],
      "metadata": {
        "id": "cjawrBXVGLbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_common_phrases(tokens, n=10, top_k=1000):\n",
        "  \"\"\"\n",
        "  Returns top_k most common phrases\n",
        "\n",
        "  Args:\n",
        "    tokens (List[]): text tokens\n",
        "    n (Int): ngram window size\n",
        "    top_k (Int): number phrases to be returned\n",
        "\n",
        "  Returns:\n",
        "    common_phrases (List[Tuple(List, Int)])\n",
        "  \"\"\"\n",
        "  # Generate n-grams (phrases of n words)\n",
        "  n_grams = ngrams(tokens, n)\n",
        "  n_gram_counts = Counter(n_grams)\n",
        "  # Get the most common phrases\n",
        "  common_phrases = n_gram_counts.most_common(top_k)\n",
        "\n",
        "  return common_phrases"
      ],
      "metadata": {
        "id": "sOqRJodgdhC9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_phrases_cleaned = get_common_phrases(clean_tokens, n=10, top_k=10)\n",
        "\n",
        "# return a dtype: List[] with the most common phrases\n",
        "common_phrases_list = []\n",
        "for tu in common_phrases_cleaned:\n",
        "  tokenized = list(tu[0])\n",
        "  common_phrases_list.append(tokenized)"
      ],
      "metadata": {
        "id": "BvpOd8GLdmej"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_phrases_cleaned[:2])\n",
        "print(common_phrases_list[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njNZiKvseOQE",
        "outputId": "d5bb74a8-d2cf-47a7-a78f-c09c5975bc9c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('for', 'the', 'three', 'and', 'six', 'months', 'ended', 'april', 'number', 'date'), 47), (('respectively', 'when', 'compared', 'to', 'the', 'same', 'periods', 'last', 'year', '.'), 26)]\n",
            "[['for', 'the', 'three', 'and', 'six', 'months', 'ended', 'april', 'number', 'date'], ['respectively', 'when', 'compared', 'to', 'the', 'same', 'periods', 'last', 'year', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Find words\n",
        "\n",
        "- rare\n",
        "- most common"
      ],
      "metadata": {
        "id": "VL97PaUOebVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rare_words(tokens, frequency_threshold=0.01):\n",
        "  \"\"\"\n",
        "  get rare words based on a frequency threshold\n",
        "\n",
        "  Args:\n",
        "    - tokens (List[string]): List of tokens\n",
        "    - frequency_threshold (float): selection of words based on occurrence count in %\n",
        "\n",
        "  Returns:\n",
        "    - bottom_words (List[List[string]]): List of rare words\n",
        "    - top_words (List[List[string]]): List of most frequent word(s)\n",
        "  \"\"\"\n",
        "  # Count word frequency\n",
        "  word_counts = Counter(tokens)\n",
        "  # Sort words by frequency (ascending)\n",
        "  sorted_counts = sorted(word_counts.values())\n",
        "  # Threshold frequency\n",
        "  percent_index_bottom = int(len(sorted_counts) * frequency_threshold)\n",
        "  percent_index_top = int(len(sorted_counts) * 1-frequency_threshold)\n",
        "\n",
        "  if percent_index_bottom < 1:\n",
        "    percent_index_bottom = 1\n",
        "  bottom_percent_words = sorted_counts[percent_index_bottom -1]\n",
        "\n",
        "  if percent_index_top < 1:\n",
        "    percent_index_top = -1 #end of list (ascending)\n",
        "  top_percent_words = sorted_counts[percent_index_top]\n",
        "\n",
        "  # Get all words with frequencies at or below\n",
        "  bottom_words = [[token] for token, count in word_counts.items() if count <= bottom_percent_words]\n",
        "  # Get all words with frequencies at or above\n",
        "  top_words = [[token] for token, count in word_counts.items() if count >= top_percent_words]\n",
        "\n",
        "  return bottom_words, top_words"
      ],
      "metadata": {
        "id": "ESvx197oeq-A"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bottom_words_cleaned, top_words_cleaned = get_rare_words(clean_tokens, frequency_threshold=0.01)\n",
        "\n",
        "print(top_words_cleaned)\n",
        "print(bottom_words_cleaned[-10:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSBc4O9fe1A-",
        "outputId": "3e1800ed-8180-4255-e09f-8b574002283a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['the']]\n",
            "[['typical'], ['methodology'], ['climate'], ['stricter'], ['adaptations'], ['noteholders'], ['severity'], ['frequency'], ['extreme'], ['weather']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Remove words and phrases"
      ],
      "metadata": {
        "id": "GwuCcVjhlulH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_be_removed = common_phrases_list + bottom_words_cleaned\n",
        "\n",
        "print(to_be_removed[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV9ew9I4lxEn",
        "outputId": "29278a57-8109-4e34-fefa-69333adfdfc9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['for', 'the', 'three', 'and', 'six', 'months', 'ended', 'april', 'number', 'date'], ['respectively', 'when', 'compared', 'to', 'the', 'same', 'periods', 'last', 'year', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def exclude_phrases_tokens(tokens, phrases):\n",
        "    \"\"\"\n",
        "    Removes tokens that belong to any phrase in `phrases`, including overlapping ones.\n",
        "    Each phrase is a list of tokens.\n",
        "\n",
        "    Args:\n",
        "      - tokens (List[string]): tokens to remove from\n",
        "      - phrases (List[List[string]]): List of strings to remove from \"tokens\"\n",
        "    \"\"\"\n",
        "    phrases = [tuple(p) for p in phrases]\n",
        "    max_len = max(len(p) for p in phrases)\n",
        "\n",
        "    n = len(tokens)\n",
        "    remove_indices = set()\n",
        "\n",
        "    for i in range(n):\n",
        "        for L in range(1, max_len + 1):\n",
        "            if i + L <= n and tuple(tokens[i:i+L]) in phrases:\n",
        "                remove_indices.update(range(i, i + L))\n",
        "\n",
        "    # Keep only tokens not marked for removal\n",
        "    return [tok for idx, tok in enumerate(tokens) if idx not in remove_indices]"
      ],
      "metadata": {
        "id": "X_3eFcvsjPQB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaned Text\n",
        "\n",
        "- save"
      ],
      "metadata": {
        "id": "92Ydk5nUy9Kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_corpus(text, path):\n",
        "  \"\"\"\n",
        "  write text to file\n",
        "\n",
        "  Args:\n",
        "    - text (string): text to be written to file\n",
        "    - path (string): path to file\n",
        "  \"\"\"\n",
        "  with open(path, \"w\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "MzXWjd3zGwJM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_path = \"/content/text-preprocessing/text_corpus_clean_generated.txt\""
      ],
      "metadata": {
        "id": "IjhUc9dxz3TW"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "partially_clean_text = create_corpus_cleaned(text, tokens=False)\n",
        "\n",
        "print(partially_clean_text[:100])\n",
        "\n",
        "cleaned_tokens = exclude_phrases_tokens(clean_tokens, to_be_removed)\n",
        "\n",
        "cleaned_text = \" \".join(cleaned_tokens)\n",
        "\n",
        "print(\"Cleaned Text:\", cleaned_text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ84CCzumY_H",
        "outputId": "545737c7-506c-4b5c-f940-a792e2d387d7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "item number . managements discussion and analysis of financial condition and results of operations u\n",
            "Cleaned Text: item number . managements discussion and analysis of financial condition and results of operations u\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "write_corpus(cleaned_text, cleaned_path)"
      ],
      "metadata": {
        "id": "Xgr7pSK_zTR5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaned Text inside DataFrame\n",
        "\n",
        "- to keep the context of the text\n",
        "- takes a long time\n",
        "- Idea: unique separator when extracting the text cells, and return them back into the DataFrame based on position and Nr. separator count"
      ],
      "metadata": {
        "id": "dcc_4tu1czXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, to_be_removed):\n",
        "  \"\"\"\n",
        "  executes preprocessing methods to clean a string\n",
        "\n",
        "  Args:\n",
        "    - text (string): text to preprocess\n",
        "    - to_be_removed (List[List[string]]): phrases and words to remove\n",
        "\n",
        "  Returns:\n",
        "    - result_text (string): output string\n",
        "  \"\"\"\n",
        "  text = remove_contact_details(text)\n",
        "  text = replace_nums_dates(text)\n",
        "\n",
        "  # lowercase:\n",
        "  text = text.lower()\n",
        "\n",
        "  # ignores case\n",
        "  text = replace_abbreviations(text)\n",
        "\n",
        "  # Or remove_stopwords_and_punctuation, does not work, out of personal experience reduces performance for some models\n",
        "  token_list = remove_punctuation(text)\n",
        "  token_list = move_s_back(token_list)\n",
        "  token_list = exclude_phrases_tokens(token_list, to_be_removed)\n",
        "\n",
        "  result_text = \" \".join(token_list)\n",
        "\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "o96MJ41PdAZV"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text(row, to_be_removed):\n",
        "  \"\"\"\n",
        "  Loops over each string column of a DataFrame row and executes a string preprocessing method\n",
        "\n",
        "  Args:\n",
        "    - row (pd.Series): row inside a Dataframe\n",
        "    - to_be_removed (List[List[string]]): List of phrases and words to remove\n",
        "\n",
        "  Returns:\n",
        "    - row (pd.Series): processed pd.Series\n",
        "  \"\"\"\n",
        "  columns = list(row.index)\n",
        "  for col in columns:\n",
        "    if pd.notna(row[col]) and col not in metadata_items:\n",
        "      row[col] = preprocess_text(row[col], to_be_removed)\n",
        "\n",
        "  return row"
      ],
      "metadata": {
        "id": "mvVgIMELegYK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply a function on every row of a pd.DataFrame\n",
        "processed_text_df = text_df.apply(lambda row: get_text(row, to_be_removed), axis=1)\n",
        "\n",
        "processed_text_df.to_csv(\"/content/text-preprocessing/processed_text_df_generated.csv\")\n",
        "\n",
        "print(processed_text_df.head(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuHOgv1Zc382",
        "outputId": "c122365d-557e-44c1-9a03-1623298316d9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   item 5  item 4  item 5.02  item 7  item 8.01 item 9.01  item 7.01  \\\n",
            "0     NaN     NaN        NaN     NaN        NaN       NaN        NaN   \n",
            "\n",
            "  item 5.07  item 2.02  item 3.02  ...  \\\n",
            "0       NaN        NaN        NaN  ...   \n",
            "\n",
            "                                      part_2_item_1a  Adj Close  \\\n",
            "0  item . risk factors risks uncertainties and ot...  76.549049   \n",
            "\n",
            "   Stock movement 1  Stock movement 7  Stock movement 30  Filing      Time  \\\n",
            "0         75.598679         77.384247            74.5989    True  16:08:06   \n",
            "\n",
            "     10  Ticker  Date  \n",
            "0  True       A    95  \n",
            "\n",
            "[1 rows x 51 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The cleaned text corpus can now be used for training\n",
        "\n",
        "- make it machine readable\n",
        "  - word2Vec (Sentence delimiters were kept)\n",
        "  - etc."
      ],
      "metadata": {
        "id": "14CkgPk9YJHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The cleaned DataFrame can be used for further processing"
      ],
      "metadata": {
        "id": "L6lBPn7kh129"
      }
    }
  ]
}